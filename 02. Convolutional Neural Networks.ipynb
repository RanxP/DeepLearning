{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff77e435",
   "metadata": {},
   "source": [
    "# 2. Convolutional Neural Networks\n",
    "Welcome to this notebook dedicated to Convolutional Neural Networks (CNNs), a cornerstone of modern deep learning. In this module, we'll delve into the motivation behind CNNs and explore their advantages over traditional Multilayer Perceptrons (MLPs).\n",
    "\n",
    "Through intuitive visual examples, we aim to demystify convolutional filters, making them accessible even to those new to the concept. By the end, you'll be equipped to implement your first CNN-based segmentation model.\n",
    "\n",
    "Let's dive in, Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde1a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# DATA_DIR = os.path.join(os.getenv('TEACHER_DIR'), 'JHS_data')\n",
    "DATA_DIR = os.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bce84a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Quadro P1000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    # torch.set_default_device('cuda')\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97380741",
   "metadata": {},
   "source": [
    "## 2.1 Limitations of general MLPs \n",
    "In this section, we'll revisit the constraints inherent in general MLPs. We'll start by defining an MLP class and then explore the consequences when attempting to increase the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2bdf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a Multi-Layer Perceptron (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, hidden_layer_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers_list = []\n",
    "        layers_list.append(nn.Flatten())      \n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            if i == 0: # create first layers\n",
    "                layers_list.append(nn.Linear(32*32*1, hidden_layer_size)) # 32*32*3 = cifar image x,y size and RGB channels\n",
    "            else:\n",
    "                layers_list.append(nn.Linear(hidden_layer_size, hidden_layer_size))\n",
    "                \n",
    "            layers_list.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers_list.append(nn.Linear(hidden_layer_size, 10))  # Assuming 10 classes for CIFAR-10\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a55c897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for 1-layer MLP: 66250\n",
      "Number of parameters for 2-layer MLP: 70410\n",
      "Number of parameters for 5-layer MLP: 82890\n",
      "Number of parameters for 10-layer MLP: 103690\n"
     ]
    }
   ],
   "source": [
    "num_hidden_layers_list = [1, 2, 5, 10]\n",
    "hidden_layer_size = 64\n",
    "\n",
    "\n",
    "# Loop through different numbers of hidden layers\n",
    "for num_hidden_layers in num_hidden_layers_list:\n",
    "    # Create and train the model\n",
    "    model = MLP(num_hidden_layers, hidden_layer_size)\n",
    "    \n",
    "    # Count the number of parameters\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"Number of parameters for {num_hidden_layers}-layer MLP: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e539d4",
   "metadata": {},
   "source": [
    "You can clearly see that the number of trainable parameters increases heavily with an increase in hidden layers. Now let's train these different models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3359d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "#             print(labels)\n",
    "            outputs = model(inputs)\n",
    "#             print(output)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs, labels  # Assuming you're using GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy on the test set: {:.2f}%'.format(accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db02b6",
   "metadata": {},
   "source": [
    "We will make use of the same dataset used in the first notebook, CIFAR-10. However to make the next part about convolutions a bit more intuitive we will transform the images to 1-channel grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba0c66dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1),# Converts to grayscale\n",
    "      \n",
    "])\n",
    "\n",
    "cifar_train = datasets.CIFAR10(root=DATA_DIR, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=10,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "cifar_test = datasets.CIFAR10(root=DATA_DIR, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=10,\n",
    "                                          shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad10fa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for 1-layer MLP: 66250\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Start model training \u001b[39;00m\n\u001b[0;32m     17\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 18\u001b[0m train_model(model, trainloader, nb_epochs, learning_rate)\n\u001b[0;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, num_epochs, lr)\u001b[0m\n\u001b[0;32m      8\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#             print(labels)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#             print(output)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m             loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\20193696\\AppData\\Local\\anaconda3\\envs\\Qquiosk-DS\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 10\n",
    "\n",
    "for num_hidden_layers in num_hidden_layers_list:\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = MLP(num_hidden_layers, hidden_layer_size)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Count the number of parameters\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"Number of parameters for {num_hidden_layers}-layer MLP: {num_params}\")\n",
    "    \n",
    "    # Start model training \n",
    "    start_time = time.time()\n",
    "    train_model(model, trainloader, nb_epochs, learning_rate)\n",
    "    end_time = time.time()\n",
    "       \n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Time taken for {num_hidden_layers}-layer MLP: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    # Test model\n",
    "    test_model(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc7fdf",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Q1. What happens to the training time when you increase the number of hidden layers?\n",
    "\n",
    "    it increases \n",
    "\n",
    "Q2. How does the loss change with an increase in hidden layers?\n",
    "\n",
    "    the innitial loss is the same, the convergence seems faster\n",
    "\n",
    "Q3. If you decrease the learning rate by a factor of 100, how does this affect the training process?\n",
    "\n",
    "    the decrease of the loss becomes slower "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae7bde",
   "metadata": {},
   "source": [
    "## 2.2 Convolutional Filters\n",
    "First, let's observe how an image transforms after applying a convolutional filter. Experiment with defining at least three general convolutional filters and observe the resulting images after applying these filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c472e670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAACyCAYAAADMBYxaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAArZklEQVR4nO2debRfVZXnv5spDJnInJiJDGRicsBCBs1iFEvUhRbSllhWN91V1Wu1XavRri66StGyXdUl1cKyq9Vua7UiJVhdKoIDSnUBSgOKQMIgIYSQ+eVlIgkZmE//ce4r3m+f7+/+9vu9l+T98vt+1sqCe96595577vDbZ+/v2cdSShBCCCGEaMYRh7oBQgghhBjeyFgQQgghRC0yFoQQQghRi4wFIYQQQtQiY0EIIYQQtchYEEIIIUQtB8xYMLNrzezrQ103cKxkZvOG4ljDETO7zsxuPtTtEBwzu8fMrh7C4/2RmfWa2R4zGz9Uxx3umNk3zOzzg9j/STNbOnQt6i70HA8Nh9NzHDIWzOzjZva4me0zs81m9hUzG1u3T0rpCyml0MM2kLqDxcwuMrO7zewFM9tuZsvM7E/M7NiDcf5DiZmtMbMLD3U7DhbV9e6vPlCbqxd35EE8/8fN7L5B7H80gP8G4OKU0kgAp5rZhiFr4BvnGW1mN5jZuqqvVlXbE4b6XAcC9kFOKS1JKd1zAM51hZndX30Lh/z4Tc6p5zh2Hj3H8XNdb2bPVL+DK8zsY632aWksmNk1AP4rgE8BGAPgLACzANxlZsc02eeogTX94GBmvwPgHwB8G8CslNJ4AB8GMB3AjCb7DMtrEWEuqz5QZwB4M4A/PbTNGRCTARwL4MmhOBh7lqt3+P8CWALg3QBGAzgbwHYAbx+K8x5m7ABwA4C/PMjn1XNcoed4SNgL4DLk3/TfA3CjmZ1du0dKqek/5A7fA+AKVz4SwBYA/7Lavg75R/hmALsBXF2V3dxvn48BWIt88/4cwBoAF/bb/+bq/2cDSNUFrAOwDcB/7nectwN4AMBOAD0A/juAY/r9PQGYR67FAKwHcE2La2bX0vScAP4GwF+7Y9wB4I+r//8TABsBvADgaQAXVOVHArgWwLPV3x4GMKP6241VW3dX5ee59vXv17MA3F+1bTmApTXX1r/PPw7g/wH4UrXvauSX6+PVubcA+L1++/42gEerNq0HcJ07dt39PQLAf6qudTuAvwcwru4+DMW//m2otv8KwI8ifVf1w+rq3jwH4Heb9P/s6pk7qtq+p3pmFgF4EcBryO/QziZt/H0AT1XnWQ3gD6ryk5Ff6FTtfzeA/QBer7b3AJhW17f92vavkN+ln5PzXw2gF8DImn5cVF3XTuQP/vv6/e0byO/Aj6pr+CWAudXfvgrgenesHwD4D8Hjfr7fvbjPHScBmAfg3wB4BcDLVZ/cQZ71Ecg/8JuqfzcAGFH9bSmADQCuQX7mewD8fuDZuhrAPQf6GdZzrOcYB/A57teO29Hqt7HFAd4N4NW+B8j97ZsAbun34L0C4APVTT8OjQbA4qoDzgVwDIDrq/p1xsL/qo5zOoCXACyq/v5W5JfjqKruU6h+mPt3Pmnvwupvs1tcM7uWpudENiQ2ATii2p4AYB+yNb0A+Yd1Wr9r63sAPwXg8aqOVdc5vvrbRwGMr853DYDNAI4lffUm5BfrPVVbL6q2J7b66CA/uK8iv+RHAvg88ov4N8gP5cXIL83Ifg/jqdV5TkN+MT8QvL9/DOBBZA/OCABfQ/XsHMh/7nqnV/19Y6u+A3ACslG0oKo7FcCSgXxkm30cSBt/G8Dc6hl4V/XsvKXJsZcC2OD2b9q3/fa/qbqm48j5bwXwzZr2HQ1gFbJhewyA86vnoq9vvoE82n478vP6dwBurf72TuTn36rtE5F/KKYFj9vyI+vrNrn3n6v6aFJ1f+8H8Bf9+vTVqs7R1fOwD8CJLe7bITEWoOdYz/EQPsfVvschGxfvrqvXKgwxAcC2lNKr5G891d/7eCCldFtK6fWU0n5X90PIltJ9KaWXAXy66qQ6PptS2p9SWo5sLZ8OACmlh1NKD6aUXk0prUF+qN7V4lh91wLkH14AgJndamY7q/jjVc2upe6cKaVfAdgF4IJq3yuRPyK9yNb4CACLzezolNKalNKzVb2rAfxZSunplFmeUtpeHfPmlNL26nx/XR1jAbmmjwL4cUrpx1Vb7wLwa+QHJcJzKaX/nVJ6DcB3kEMxn0spvZRS+hmylTuvatM9KaXHq/M8BuAWvNHvre7vHyB7hzaklF5C/lB96CCFeG4zsxfwhrfkM1V5q757HcApZnZcSqknpTQkLlRPSulHKaVnq2fgXgA/A3DeAA4R6dvrUkp7yXsJZKO0p+b4ZyF7Ev8ypfRySumfAPwQwL/oV+d7KaVfVd+Jv0N2lQPAL5Cfg77r+RDyu7UpeNyh4neRn+stKaWtAD4LoP/7/kr191dSSj9GNnzZ+3Yo0XOs5/hAPcdfRf6N/WldpVbGwjYAE5p81KdWf+9jfc1xpvX/e0ppH7L1W8fmfv+/D/mGwMxONrMfVkKf3QC+gEajpRl955varx1XppTGAngEeXTdR8O1BM75TeSXFtV/v1UdfxWyxXwdgC2VcTKtqjcD2eVWYGbXmNlTZrbLzHYix5XYNc4C8DuVwbOzqntu/2tsQW+//99ftdmX9fX7b1XC0K1mtgvAH/ZrU6v7OwvA9/u18SlkQ2pysJ2D4QMppVHIlvfCfm1u2ncppb3IWpY/BNBjZj8ys4UHonFmdqmZPWhmO6o2vAex57mPSN/WvZvbUf+8TAOwPqX0er+ytcgj2j7ou5rysOVWvPHh/AjyRzh63KFiWnXs/ueZ1m97uxsQ/fM1DCP0HOs5HvLn2My+COAUZKlB7QC+lbHwAHII4HJ3ghMAXIosKOmj7kQ9yO6lvv2PQ7YE2+ErAFYAmJ9SGo3s/rHAfiuQtQOXt6qI8lpanfNmAO83s9OR41e3/fOBUvp2Sulc5JchIYtFgfzgz/UnNrPzkHUOVyC7kMYiey7YNa4H8K2U0th+/05IKR0I8dW3keNaM1JKY5Ct0b42tbq/6wFc6tp5bEpp4wFoJ6Ua7XwDOUTS16amfZdS+mlK6SLkD9AK5LAYkOOvx/c79JS609a1ycxGAPhu1abJ1b3+MZo/z+x4kb6ta8c/ArikeqcZmwDMMLP+34qZyO9ShFuQR4izAPwW8vUO9LgNfW5mvs9beSk3Ib9//c+zqXXThx96jpui53iAmNlnkX/HL04p7W5Vv9ZYSCntQnZ1fNnM3m1mR5vZbAD/B1lM8a1gu/4BwGVmdnalWv0sYj/wjFHIcbg9lZX8R5GdKqvpGgCfMbN/bWYnWmY+Wo9wa8+ZUtoA4CHk/vhun5vMzBaY2fnVy/Qi8kj9tWq3rwP4CzObX7XjNMvzj0chx562AjjKzD6NLDRl3Izcr5eY2ZFmdqyZLTWz6U3qD4ZRAHaklF40s7cjW9d9tLq/XwXwX6oXDWY20czefwDa2IobAFxkZmegpu/MbLKZva/68LyE7M7ru2/LALzTzGaa2RjUq9J7AUy3JrOGkGOcI5Dv9atmdimyVqTueOOr8/Yx2L79FvKH+rtmttDMjjCz8ZZzn7wHWei1F8B/rN7/pcgq6lsjB08pPVpd39cB/DSltLP600COuxzAEjM7w/IU5+vc33sBzKlpxi0A/qzqmwnIYbK2cpX0PSvIce0jqufm6HaONQhugJ5jj57jAWBmf4r8Db8oVeHvVrScOplS+ivkkfT1yD+Yv0S+KRdUsaWWVHGyf4fcgT3IApAtyA/wQPkk8kW+gGwlfye6Y0rpO8gj9o8iX8M2ZNXt/0Q2gAZzzm8iCwD7G1AjkKdYbUN2cU1C7ksgzzv+e+TY3m4Af4ssNPkpgJ8AWInsZnoRTdxvKaX1AN5fHXNrVe9TODDJtv4tgM9Zjpt+ump7Xzta3d8bkb0SP6v2fxDZOj+oVHG+mwD8eYu+OwLZsNyELHp6F/L1o4oJfwfAY8gzVX5Yc8p/QlZHbzazbf6PKaUXAHwCuS+fR37Gbq9p/wrkD8bqyl07DYPs2+odvhB51HkX8rP4K2QX8i9T1qC8D3kEsg3A/wDwsaotUW6pzvHtfucNHzeltBJZuPWPAJ4B4Of8/y2yLminmd1Gzv955Dj+Y8jiwEeqsna4Ctno/wpyDHs/3hitHxT0HNNj6jkeGF9A9kw8YzknxR4zu7Zuhz5150HFckKRnchu/ecOegMOAGb2TmQrb7aLX3Udh+P9FUKIbuagrQ1hZpeZ2fGVS+x6ZMtozcE6/4GkckP+ewBf71ZD4XC+v0II0e0czIWk3o83kknMB3BlK/VlJ2Bmi5BH0VORY4ndymF5f4UQQhyiMIQQQgghOgctUS2EEEKIWmQsCCGEEKKWcLrdO++8s4hXvP56o5bv2GPLVZ6POaZxau6RRx5Z1GGhEDMbcB1W74gjSnsoEnqJ1HnllVda1gF4O1udLxoe8vcgup+v548DAK+99lpRFiGy33vf+95282y0zSc/+cmic3bs2FHU27lzZ1HGrsk/2wBw9NHllHt2T3x/v/zyy6FzRu8v23fEiBFF2fjxZW60SZMmFWXHH398UcberaGGPZftEm1v5LsCANdee+1Bf4YvuuiioiGRb9z+/WWG5FdfZVn82+Ooo8qfEvYuRL6ZbD8GO5bvi+i9ZES+26xOZD+gbCu7j6ys3XOy39477rgj/AzLsyCEEEKIWmQsCCGEEKIWGQtCCCGEqEXGghBCCCFqCQscGV7gxQQz+/bta9g+4YRyUbCooCXCSy81LjfBhD3HHXdcwzYTUfl2A6XYZOTIcvVPJjTx/dKO4LEZEQFYRGDH6rQrcIyIJw8Fjz/+eFHGRFKsvezZjgrEIuJc9g6wMiaqZOIyJmZ68cUXi7KNG8vF8V544YWijIkex40bV5QxEWVUZMzKIqLEqKBsqPc9FLDvp/+eAcCYMWMatlnfsvsceaajQjwmio18H9lzz8Tz27YVy1Rg8+bNDdvs/WbvRrtiRkZUlBgRYw6liHiwz7o8C0IIIYSoRcaCEEIIIWqRsSCEEEKIWsKaBRbf8vGgrVu3FnV8TJTFNEeNGlU2zMVhWbyFxaN8/Iu127eBHZtpHXzinDlz5hR15s+fX5RFNBKRuH67MadofLgV0fjZcNEoCCGEGBoGJXAUopPwoq9mZUy4ykRRzFiNZvX04tGoIRgVQjJBGMsSyURivb29RRkbCLB+YhkhTzzxxNC+TLw5lAKvqNE8nI3dhQsXFmWsj0aPHt2wPXbs2KKOF4M3K/P9wc7H3g/Wj0yMy87pYWLJk08+uShbt25dw/aqVauKOtEMrZFnbygzLA53FIYQQgghRC0yFoQQQghRi4wFIYQQQtQS1izcf//9RdmePXsatln8xieCYTFdFl/1RJMG+Xrs2D5uxkSXjL179zZsL1u2rKjDYrtz585t2J4wYUJRx8fk2PWyhCntJmVqh2gs15+v3eROQohGZs2aVZRFVjplmgL23Yus5Mi+8+w7y+L0TDfjNQusDvuGMR3DokWLGranTp1a1Onp6SnKNm3aVJTt2rWrYZslNWNENQu+bDBJnyLnG6z+RwJH0TUsXry4KIuK6qJlzKBqd7lz9tFkH6xo5kMm5mQZAdnsJJ8ZDyg/pgCwdu3aoowZ0BMnTizKmBHthXrRJe6Hs0hRiE5EYQghhBBC1CJjQQghhBC1DCopk3f1sdi0d/Myty9zLXo3anRxH6+jYK5cfz7mio3E+LyGAeBuWJ+YyrtWAWDmzJkN28wlyxbu8e1sNwFTRP8R1T60u58QQojhiTQLQgjRIUTF2F7g1q5AnMHqsIy3TPvCyvw1MV0OG/QxnY/PlssGWCxJmB+sAcD27dsbtpluhwkjWVsjSZ+iosR2xZKDTQwlY0F0Dcw7xT44Q60kZh4wfzyW2Y6pvVk7oh8n1g52XqYgZ4JE78UDeHY8liWSlT3//PNFmRdlRrNwsh9ViSOFaB9pFoQQQghRi4wFIYQQQtQSDkOwxT5YMhCPd322K3ZjMTGGd90yV6kXJkaEmUCZeIS5NSOLojAXrHffstjetGnTijK/8iVLjsJcst51HXG9tptDQG5dIYTobKRZEEKIDsEL+ACuRfFlg1le3mt4mJaGtSEqevTHY4Mwpq2JaHWiOhW2Cqrv6ylTphR1mL6HCSF37NhRlPmBbHRQFdFUSeAoxCCIekYGs+ws25d96HxbWNvYh45N82XepMhSw83K2HUxTxtTlbMfA1bPK82bla1fv75hm6XqZVOR2VLZrG3RqdyHwxLDQgwGaRaEEEIIUYuMBSGEEELUEg5DRBJlMFcdi7ENFSyrpHfTMhewn1sfXZzH0667krXJw65txYoVRZkXS7J44vTp04syn6wkkuwl6sb2fSeBoxBCdDbSLAghRIfANBaszA+IIiJIIDbYYQMpNihkgzBW5veNaHwArg9i19QufjDIzscyP06ePLkoYxoir7/p7e0t6uzbt68oiyR5i66SOxBkLIiuIfJRHUhZVMzIzuunHUdFlcybxaYwRzxqQDzlL9s3KhhlPySTJk0qysaOHVuUeRU5yxAZzRrJ+omJQ1l7o2mWhThckWZBCCGEELWEPQtsBOLL2Kgi4tZqd04oGyl4HQWr40cOUc2Cd/9Ec+1HVmGMxPnZ6HPr1q0N22zu8XPPPVeU+VUtZ8yYUdTxc4/ZKIzdp8hqpEIIIToHhSGEEKJDiMaifegrumojC3N5Y58Z/+z4kWMBZfKmyIALiOUDicT3hxrWfhZi86E4Jmpni6v5DMRAOeBl97bd7Ml9KAwhhBBCiFrkWRBdTXSUwSx1NmphIyxWzwvm2ChsMFNO2Sgiumz1wZjqyvqdCQt9eIyl3N29e3dRxkSPrB5TqbORmxDdjjwLQgghhKgl7FmIxDvYKGXXrl0N22waGcNb/GwkwsSLvh5rtx/FMeEewx87OlKLiDX9aC6ai95fH9uPLejic+5v3LixqONHvyzhU2SVy8jqpEIIIYYvCkMIIUSHwMRzkQXN2AJkLNzEZoZFZjdFc25E2++JDjj8sdh+0cXT2hnkDQR/j3xWXYAvwsbO6QeprM5gZ6XJWBBdQ9TrE5na2mxf9nFi3rR2VzFkMfZociiWWCiqWI/2E2Mw/emvjXmyWBn78DI9iV8mGOCrXzINhBDdhDQLQgghhKhFxoIQQgghagmHISLuz0jcisXEmADPu26jAkfmVvW0m2HQT59j+7Hz+7ZH4mNR123kelnfRfrAXy+besbueWQ1UiGEEJ2DNAtCCNEhMN0JG0T4AQLbLyqC82VsgMIGk6xdrB2+zC8eBvCBIRNt+rZGBkrN8IOc6KAnujhbu8dn/e91O2PGjAntNxBkLIiugb0s0SV/2Ysc/ShEvGLRpEzsnOxD6tf1AOJre0RXp2QeQbakLiuLfrAj689E+5zdV5YIiqXmlXdMdDvSLAghhBCilrBngWkN/GiIWfh+BMEWy2CuqVGjRjVsR+fBRtad94tzsPSuc+bMKcq8q40t8sFGb2yk5omsOhnRhESnuPlphBE3GZt6xnQMPT09LY8thBCic5BnQQghhBC1SLMghBAdAvOcMnGhL2NeSabhYCndvfeQJbKKZnCM6HCYfmfz5s1FGesL3/6oxoeJJX296KJzzLvLkqR57y677mgWTJ/pkeluxo8fX5QNBBkLomtgH0IGe0HZB5kJ91i4jn2UI+uRsGOxa2DhoS1btrQ8PsA/gNGpxOyDxD7ETETIxJEsw6bv9+iqntEMlqyM3ZvRo0cXZUJ0EwpDCCGEEKKWsGeBifR8mRclAuVIiE3pYkRWnZw0aVJR5kcUbNTh28lGSGwk4a9l8uTJRZ2Im431pXezsZEsw58vOir2I8CIgJQdh7kHfR59JiAVQgjROcizIIQQQohapFkQQogOIZJhESgFe0zrwbyQTBPjvZfMU8h0HlFxnq/HPNRMv8Pa4a+b9c2uXbuKMjalf9q0aQ3bTHvEPMnRlVE9zHvOrpvdSy+OZILNSFqBOmQsiK6BhVpYmIq9oGxfpl6eMGFCaF9fxj587PjRXBesjO3LBInsA8g+ZKzN06dPL8pY6JEdb+vWrUWZV94zUSm7LnYN7EeIpcU9EB9aITqdsLEQefnYS7Zp06aG7egH268pP2XKlKLOm970pqJszZo1DdvMuo1oLVavXl2UeY0CsxZXrlxZlPl6J510UlHHX9+6deuKOkwz4C11Nq3JawiA8uPH1OletxH96Leb81wIIcTwRJoFIYQQQtSiMIQQQnQILOTCdADeUzgYTYGP+zNdA/NoshBPJEV/JP0/wHUG/ppYWIyF6J599tmizHt3mUc4MmsO4O33+UeYl5Z53Xt7e4syvy8LL0bzpzRDngUhhBBC1CLPgugamGAwmumP6XGiIwgmmPRCPaa0jmpEGEwIGNGuAHGNCRuhsfNG+45drx9Js/vFYH3OdFdsRMyI5ocR4nBlUAJHn8xo27ZtRR3/EWSpXyMpZ0899dRQm/xHKPJxZR8M9tHzbi9Wh7ml/OqULJGR/xjNnz+/qLNq1aqizPc5Oza7Pt+/7Fr8/Z09e3ZRhwkjfVn0B04IIcTwRF9xIYQQQtSiMIQQQnQIbPp0RDTIPIdM4Oi9oGxf5k1kgjrWVpabw7efpfFnAk0W3vKhMXY9zLs9ceLEosxf99NPP13Uectb3lKUsZBVJBTHrpGF3Vg9368s5MpCnQNBngUhhBBC1CLPguga2CiACfyiS0+zfdn0pEgmRqZ1YSMBdk5Wjx0vIiAE+EiVjRLZCNMnUwN437HR49SpU4uyE088sWGbTZeLpjJmC7ixeuz6pbsR3U7YWGArM/p5tDt27CjqeJcJU0GzuaQLFy5s2J45c2ZRh82N9cdnHwifkpel6GV4V9KWLVuKOizTpP/QsB8U33fsw+n7BAB+/vOfN2yzDzNzZUVWq/Q51Dds2FDUYT8Y/sdxsPN7hRBCHFpkLgshhBCiFoUhhBCiQ1i+fHlRxsImfh0bRjTvhPfOMpEiCysxjyLb14f9mDeYhdVYaMh7f1lYjLWV5Ufx3t2NGzcWdZ566qlQW8eNG1eURXKGsBAbC0X6vjgQC5/JsyCEEEKIWsKeBRZDv+yyyxq2n3vuuaKOtySZUIqVeY0Cs4JZ3m8fQ2eWrF+tkh2bidz89KDI0sNAKdLq6ekp6vhkSsyiZNe7aNGilvv5lT+Bsl9YH7DpPh5mHUez4h1s2CqlM2bMCO3LtCCsjI1kWH94DRATKbJ7yZ5LpvlhMG0OaxsrY+dgzzqb1sVEiWwkN2vWrKLM3zOmkWHaIa+3AXjfRYWbEjiKbkdvgBBCCCFqkbEghBBCiFokcBRCiA5hxYoVRRnLLOgFjizMxcpYGMmHpVi2RhYKYtPkWTjHCxrZAmVsSjoLnfpMjD4EDPA1jFiYffz48bXtBID169cXZffee29RdvLJJ7csY1kkGUw46vuVTYUfbChNngUhhBBC1BL2LDAr9G1ve1vD9mmnnVbU8UK6aGY1Lx5jYjImTvL1mMDRW8Zs6V6Wdc8fe9q0aUUdJl70q0WyqTu9vb0N20wUxq7FC/Tmzp1b1GFTfpYtW9awzUSQ3hJl94kJ3/w9ZiORQ8EvfvGLooyJ6piQlAny2PPH+ogJAf3xmJiUCfIY7J2K5pRnYsbodLNoNkn/bANxAaIXeZ500klFHbYiLXt/1q5dW5StXr26KGPXz+6rEN2EPAtCCCGEqEWaBSGE6BB8in2AewW9FyU6zZt5uPyaIsybyLxWrIx56LwXjHmn/NRygK8e6fUIzNPKPLRMe+B1DOy6WRuYfuOee+4pyh599NGGbeblnD17dlHGtA3e88f0HKxsIMizIIQQQohaZCwIIYQQopZwGOL5558vyrw4iGXD8yJA5rZh7jAvMmLn91NbgDL7GsvG5kVdTAzFprr4/dixmZvNu6/OOuuslsdmrkXWT154xdyIp5xySlG2ZMmShm0mOPPCzCeffLKo88wzzxRl3o3IxKKHgscee6woY9fExHws+yGbGsaulZX5PmLLR0eWtgb4c8H2ZUJTtoYAW+E1ejzWPuY2XbNmTVHGxIb++WL9xNrLRKrse8FW02XfGvY+CtFNyLMghBBCiFokcBRCiA6BJRliIjg/XXrHjh1FHSb0Y14Vf07mZWJTcKPTd/36H0xIyKYpMy+T9/Y9++yzRR2W4IlNeffHZ1445sll94i11XumWcKtlStXFmXMQzZ//vyG7TPOOKOow7xoTEzaDHkWhBBCCFFL2LPALChmrXq8FcfilyzO76cIsbgsixn7Y0XSi0bj1v58zDL3Fh5QahRYghdfxkYLLFmMt4g3bNhQ1GFWuY83sylZ8+bNa9hm05DYSqMPPPBAbRuFEEJ0FgpDiK6BuQIjxg7ADePRo0cXZQsXLizK2NLYXugbnfPOyphomNVjmUrZNTAxZ0RcC3CDlmVmZPsy96039lk20ieeeKIoYzD3MxP/MsEku4dCdBMKQwghhBCiFnkWhBCiQ2DiPxba9V4U5mli00GZoM5Ph2VTZpl3i62OyKab+ymzzMPEVqJkWR29gJKtB8I8jJFpxazvGUyAyNYR8hkbWR+y/mJ94df6YX1zzjnnFGWLFi0qypohz4IQQgghagl7Fpj16i0tlhN7+fLlDdssMQ6LBy5durRhm01bYTFYP12HCRx9nUhMGSinmbBpQGx6iu8nFsP1ddg0Ghbn9cmj2LrsTPTorXk2xcj3Hbs2lvzmvPPOa9hmGgAhhBCdg8IQomtgLlbmpmSGk1+gphksEyYT1nmjLpqFkRlsbDYRM3aZscmMaTbLiB2PLY3NsjCyfmdLzvsl7wHgrW99a8M2m5HDBinsvrLBxUMPPVSUbd26tSiTwFF0OwpDCCGEEKIWeRaEEKJDYB4Olj3Re7NY3hYmnmPr5Nx1110N22waMRP/sTKW1dGfk3nAogJKD1uWmXn/WF6dxYsXN2yzJaRZfzFvFcs148WFLNTOypgn0bd/3bp1RZ2HH364ZRvqCBsLjzzySFHmO33cuHFFHa9RePrpp4s6559/flF20003NWxfcsklRZ2Iu5Sls/QvCZtXzh5OltzIw9zJHvbC+Pn+X/ziF4s6zN36jne8o2H7wx/+cFGHucEjdXp7exu2mcuarQPvP0wLFixoeX4hhBDDF4UhhBBCCFGLwhCia/CeEoCL9PxsDgA488wzi7IRI0YUZSwj5K9//euizC//zVyLDCa+Y3PEWdtYveiS2qyMuVaZ+5N5xJh7ODKn3AseAT6vnblXmXubCVfZMttRgasQhysyFoQQokNg6cTZtHZvGDJDac2aNUUZC0deeeWVDdssJs9mt7Ap4sxI9NoGZhAzA46FfL3xz0KnDDYY8AanD/kCfHYSM9TvvvvuoszP2GFtZQY0S7Dlz8kSW7FETQNBYQghhBBC1BL2LLB1zr3LkS3y4ssuvPDCos5nPvOZouzGG29s2P7BD35Q1GHqVG9RM/euV9EyK4yJHr0rllmQzDr0VjGziL/85S83bLOVMJlY8/vf/37Dtk/SBHCXrE8oxUYefiXKqJXuRwozZswI7SeEEGJ4Is+CEEIIIWqRZkF0DSzmyGKtzGN1+umnF2XMI3XfffcVZUwI6D1HzLPDRIUs5huFpSdn3jE2tZedl3kb58yZU5RdccUVRRmbP84yPfqpuY8++mhRhy3Sw+b47927tyhjfcyEoEJ0OzIWhBCiQ2CGTERkx2YCsdTcDz74YFH2iU98omGbhZKZKJHlpWFCRS9KZEmTWBkzsP3xmdHI8vOw8K2fZcPSwL/5zW8uyi699NKijM2S8sYru7dMvMoMXCYm9URyANWhMIQQQgghagl7Fli6UO+aZJaed31GF2Tx9W677baiDpu/7TMvskyMHmbRsekp3iJlgsPI/HaWIvQ3v/lNw/bFF19c1GFW7Ne+9rWG7Xvvvbeow/rc3xc2P/6JJ55o2GZ9yVzx3spl7m8hhBCdgzwLQgghhKhFmgXRNTAhHBPusZggW4SFxUOZmJF5qfx0UhbfZQvlsHrMo8fqsbgmmw7LBI7sWtn0Ypbc5iMf+UhRNm/evKLML1gEAD/5yU8atlkiIRYHZ7A+YdfKPI3bt28PnUOIwxUZC0II0SH49NcAN3i9mI2J25gxykKNXoC4ZMmSog4zpli7mGHr28GMv2iGSL8vOx8TOLJw6qZNmxq22Uynxx9/vCi76qqrirKzzz67KPMzoli4li3OyAStXizJMj9GQvJ1hI0FduP9TWbxep/Yhy2nypS6fhlUlqqStcl3eGTUwbQH7Mb5UQjrfLaff1HZtfgX8oMf/GBR59xzzy3K/NSy733ve0UdtmKoH2Wxe+CnxrHRJcPvJ82CEEJ0NtIsCCGEEKIWGQtCCCGEqEWaBdE1+BgkwMVsLBbKpruyOC2Lh7IwzIIFC1oen8WnWRZCFpdlZSyMxMrYst1MCMjiqSykx5LZsJgqS+zjQ43sHrLjs2WrWayXtZfdCyG6HRkLQgjRITBjNLJUMzPsWP4VZij6c7LZPawNTNPFdG1e+8YM2Kjx67VYrA4bILBZQatWrWrYZkYp03otXbq0KDvnnHOKskmTJjVss0UPWRkzer0AdOfOnaFjDYSwseAFh0B5A1mH+5HBsmXLijos2ZCvxx5idj7/sLCRiK/DHh42fc7DRiXsWL4P2KjKT0Nj09LYSHPq1KkN2+w+3XnnnUWZH6GyUbJ/iaNCRZ+idMqUKaH9hBBCDE+kWRBCCCFELQpDiK4hmuSIrabIFoJhMXbmEWIrMY4ePbppO/tgbtzo3PWoKze6L/PiMV0AW/CHzU9nHjg2l96XMXc6O+fIkSOLMuY+Z33MvIpMsyFENyHPghBCCCFqkbkshBAdAtMNRRLIMc/I5ZdfXpStXbu2KJs+fXrDNhPYRUWDbF/vyWEeQOZRY/h92bGYGJMl74totHzSQYB7yFiqee/9YuLPiCAUKD2k0USDA2FQxoJ/GJhwzwvp2EqRX/rSl4oyrzxlN4CtlOgfKtax3vXKHhT2QEVSqEZgYk3vImWCQ/bgeBcsc6szF3qkn/wHgLWbtcmrpyNrrQshhBi+KAwhhBBCiFoUhhBdw+zZs4syJuZjUz2ZW4+5Fpmrj53XC+uYm5Wdk7mTmVeIecuiZUz0GJnLD/D53dEVMJl40XsvFy9eHGobEy4yVzmDPRPMaypENyFjQQghOgRmZDHjJmKM+qRAANcU+DIWEo4udc6MLm/EsWMxgzCyvHjUyGXh1Llz5zZss9k/bCDABgzM8PcrXUavmxnz/v6yBExsUDEQwsYCs/o97EH2Ig528Wyqmp+Cxs7PLj6SDcyXRZZNZWVsxMReSv9wsj7w2gM23ez8888vyp544omW54+OFD2+n6LL3HqRFHtRhBBCdA7SLAghhBCiFhkLQgghhKhFmgXRNbCYIMv0x0Jefq45wMMrrIxlDvRtYXFUVhZlMKJHFl5j4TwWdly+fHlRtnr16qJs48aNRdm8efOKspNOOqlhm8XUo7A+YdcVXdlTiG5CxoIQQnQIkZTYQGnIsRTWLF05W9UysmQ3M5KjokffNtZWNpMlksSInS/arjPOOKNh+4orrijqMGObCUejAs0IkYUPWd9EZwM1Y1DGghfTRbJGsQ7yqlCgHFUx4V5k1Um2XyTLF8OPLvbs2VPUiYze2GjG17n99tuLOl7MCAAPPfRQwzbrE4bvl8goNppFzbeBTWMTQgjROUizIIQQQohaZCwIIYQQohZpFkTXwGKvLI7HwmksBhjJqQHwMJcP1UTDRyz2GY3Bsng0S97CzhEJ5zVrCxMHsiW6p06dWpQtXLiw5X4srMfay8rYvuweRkNwQhyuDMpYiCw05F8y9nGOfGCZ6IV91CIiDv/xin70fRsmTpzY8thAeS2RDxlTZPf09BRlPoMYU6hHFnKKtIn1Cftx8P3U7oJbQohG2HsaSZYWTSDHvhWRpHIM9s2OCBzZNyX67Ylo5CICQaDUWp155plFHSYSZTOsdu/eXZR5IosXAvy6I9/qwRq8CkMIIYQQohYZC0IIIYSoRcaCEEIIIWqRwFF0NUzgxnQnLJYb0WsAsbjjYI4VzfTIjhddBjsaf2Yr811wwQWhfdlqfWPGjCnKPOz6o3H1aOZMZXAU3c6gjIWI8MV/BJnIgglO/MckKv7w+0VEMCyRElN6+/NFryWS2tfXYSKZmTNnFmW+DexaIkuatiuaYT8iEXGVEGJoYN+qyLvLDGVG5LsXJSJKZ98UBrsmLwxnRmm7gmvWLmbgrly5MlTPG+pRozc6c8rD2n/qqaeG91cYQgghhBC1yFgQQgghRC0yFoQQQghRiwSOomvYv39/URaN/0UFiNGkORG9T0T/0qwdDNa2qHAvmuSFtY+JKFmb2f3ZuXNnw3Y0oQ5jqPvuUMBi8JGEalHNV1TH4IkKRSOC4mhbGf4eR1ewjJyT6cGY8JmdM1IW/Ra1e48Gq1kIGwtsGdTIjfFl7MGOKK2jD6OHPQQ+dS+7Se2KSBj+5rKHzvcLeyCYUNCLm9pVbUfS3kaV8x6WZlgIIUTnoDCEEEIIIWqRsSCEEEKIWmQsCCGEEKKWIRU4RlZcZMkp2H4+hs5i45EkFu0mLIkkToqKbnysPxLnb3eJXaY9iC7h6/F9165uZCj1H4Mhunx0NEtidF9GJFkZ06hEl6NmOqCoNieaOTIqtmMisEiiMHbeaP9GV5KNEhVCHmiife5hOqmo0C+ig2o3aR5QvkeRBHFA+/cz+m74etH92l0pkn1PoplTI/eIHX8gDI+vuBBCCCGGLTIWhBBCCFGLjAUhhBBC1CJjQQghhBC12GBEP0IIIYQ4/JFnQQghhBC1yFgQQgghRC0yFoQQQghRi4wFIYQQQtQiY0EIIYQQtchYEEIIIUQt/x+tDZnxaxORWgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 648x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up a transform to convert images to tensors and grayscale\n",
    "transform = transforms.Compose([    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1), # Converts to grayscale\n",
    "      \n",
    "])\n",
    "\n",
    "# Load a single grayscale image\n",
    "cifar_train = datasets.CIFAR10(root=DATA_DIR, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "image, label = cifar_train[4]\n",
    "gray_image = image.squeeze().numpy()\n",
    "\n",
    "# Function to apply 2D convolution with multiple filters\n",
    "def apply_convolutions(image, kernels):\n",
    "    image_tensor = torch.tensor(image).unsqueeze(0).unsqueeze(0).float()\n",
    "    results = [F.conv2d(image_tensor, kernel.unsqueeze(0).unsqueeze(0).float(), padding=0).squeeze().numpy() for kernel in kernels]\n",
    "    return results\n",
    "\n",
    "# Function to display images\n",
    "def display_images(images, titles):\n",
    "    plt.figure(figsize=(len(images) * 3, 3))\n",
    "    for i, (img, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(1, len(images), i)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Define multiple convolutional filters (kernels), these should be a 2x2 or 3x3 pytorch tensor\n",
    "kernels = [    \n",
    "    torch.tensor([[-1,-1,-1],[0,0,0],[1,1,1]]),\n",
    "    torch.tensor([[-1,0],[0,-1]])\n",
    "]\n",
    "\n",
    "\n",
    "# Apply 2D convolution with multiple filters\n",
    "results = apply_convolutions(gray_image, kernels)\n",
    "\n",
    "# Display original image and results\n",
    "display_images([gray_image] + results, ['Original Grayscale Image'] + ['Result after Convolution {}'.format(i+1) for i in range(len(results))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a31ae7",
   "metadata": {},
   "source": [
    "Now, attempt to implement a basic CNN architecture. Refer to the lecture slides for guidance on its structure and components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65377f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleCNN model, for visualization purpose please refer to the first convolutional layer as conv1.\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # TODO: (1) make a CNN classification model consiting out of at least 3 convolutional building blocks. (2) the first convolutional\n",
    "        # layers should be refered to as self.conv1 for visualization purpose. (3) the output layer that returns the output \n",
    "        # for the 10 classes (NOTE: we flatten the images; see forward function)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels =1, out_channels = 64,kernel_size=(3,3),padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels =64, out_channels = 64,kernel_size=(3,3),padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 1,kernel_size=(1,1))\n",
    "        \n",
    "        self.l1 = nn.Linear(1024, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        flatten = nn.Flatten()\n",
    "        x = flatten(x)\n",
    "        \n",
    "#         x = F.relu(self.l1(x))\n",
    "        output = self.l1(x)\n",
    "         \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e8c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for CNN: 410243\n",
      "Epoch 1/10, Loss: 1.9713\n"
     ]
    }
   ],
   "source": [
    "simple_cnn = SimpleCNN()\n",
    "nb_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create and train the model\n",
    "num_params = count_parameters(simple_cnn)\n",
    "print(f\"Number of parameters for CNN: {num_params}\")\n",
    "\n",
    "# Start model training \n",
    "train_model(simple_cnn, trainloader, nb_epochs, learning_rate)\n",
    "test_model(simple_cnn, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "905cda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(model, layer_name, num_filters=16):\n",
    "    \"\"\"\n",
    "    Visualize filters from a specific convolutional layer in the model.\n",
    "\n",
    "    Parameters:\n",
    "        - model: The CNN model\n",
    "        - layer_name: The name of the convolutional layer (e.g., 'conv1', 'conv2', 'conv3')\n",
    "        - num_filters: Number of filters to visualize\n",
    "\n",
    "    Returns:\n",
    "        None (displays the visualizations)\n",
    "    \"\"\"\n",
    "    # Get the specified convolutional layer\n",
    "    layer = getattr(model, layer_name)\n",
    "\n",
    "    # Get the weights from the layer\n",
    "    filters = layer.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Get min and max value of all filters to scale plots\n",
    "    min_value = filters.min()\n",
    "    max_value = filters.max()\n",
    "\n",
    "    # Plot the filters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(num_filters):\n",
    "        plt.subplot(2, num_filters // 2, i + 1)\n",
    "        plt.imshow(filters[i][0], cmap='gray', vmin=min_value, vmax=max_value)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Filter {i + 1}')\n",
    "\n",
    "    plt.suptitle(f'Filters from {layer_name}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.eval()\n",
    "\n",
    "# Visualize filters from the first convolutional layer\n",
    "visualize_filters(simple_cnn, 'conv1', num_filters=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30361631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters_to_image(model, image, layer_name):\n",
    "    \"\"\"\n",
    "    Apply individual filters from a specific convolutional layer to an input image.\n",
    "\n",
    "    Parameters:\n",
    "        - model: The CNN model\n",
    "        - image: Input image (NumPy array)\n",
    "        - layer_name: The name of the convolutional layer (e.g., 'conv1', 'conv2', 'conv3')\n",
    "\n",
    "    Returns:\n",
    "        None (displays the visualizations)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the specified convolutional layer\n",
    "    layer = getattr(model, layer_name)\n",
    "\n",
    "    # Get the individual filters\n",
    "    filters = layer.weight.data\n",
    "\n",
    "    # Apply and visualize each filter on the input image\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(filters.shape[0]):\n",
    "        output = F.relu(F.conv2d(image, filters[i].unsqueeze(0).float(), padding=1))\n",
    "        plt.subplot(2, filters.shape[0] // 2, i + 1)\n",
    "        plt.imshow(output.squeeze().detach().numpy(), cmap='gray')\n",
    "        plt.title(f'Filter {i + 1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f'Application of Filters from {layer_name} to Input Image', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Load a single grayscale image\n",
    "cifar_train = datasets.CIFAR10(root=DATA_DIR, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "sample_image, _ = cifar_train[4]\n",
    "\n",
    "# Apply and visualize filters\n",
    "apply_filters_to_image(simple_cnn, sample_image, 'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8a05d",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Q4. How does the basic CNN model performance compare against a MLP based model with comparable number of trainable parameters?\n",
    "\n",
    "Q5. What happens to the loss from the CNN model compared to the MLP based models?\n",
    "\n",
    "Q6. What can you tell about the learned filters in the first convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eeb13a",
   "metadata": {},
   "source": [
    "# 2.3 Segmentation Models\n",
    "Let's transition from the CIFAR-10 classification dataset to the VOC2012 segmentation dataset. In this section of the notebook, our attention shifts towards semantic segmentation, where we'll explore techniques for delineating objects within images. Due to computational constraints, we'll be training our model on a CPU basis, necessitating a resize of the images to 32x32 pixels. While this resolution may limit the quality of segmentation results, this exercise serves as a valuable opportunity to grasp the fundamentals of building and enhancing a segmentation model. If you have access to a GPU your own your ofcourse free to try and train a segmentation model on a higher resolution with more epochs and get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for input images and masks\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "target_transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32),transforms.InterpolationMode.NEAREST),  # Resize masks\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load PASCAL VOC dataset\n",
    "train_dataset = datasets.VOCSegmentation(root=DATA_DIR, year='2012', image_set='train', download=False, transform=transform, target_transform=target_transforms)\n",
    "val_dataset = datasets.VOCSegmentation(root=DATA_DIR, year='2012', image_set='val', download=False, transform=transform, target_transform=target_transforms)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8, pin_memory=True)\n",
    "validationloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Segmentation model\n",
    "class SegmentationCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SegmentationCNN, self).__init__()\n",
    "        # TODO: Implement a convolutional neural network for image segmentation.\n",
    "        # (1) Design the layers to extract features from input images.\n",
    "        # (2) Ensure the final layer outputs a tensor of shape [B, 1, X, Y],\n",
    "        #     where B is the batch size, and X and Y are the dimensions of the output segmentation map.\n",
    "        #     This indicates the segmentation prediction for each pixel in the input image.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622207e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_segmentation(model, train_loader, num_epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, masks in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            masks = (masks * 255)\n",
    "\n",
    "            loss = criterion(outputs, masks.long().squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283ba7a",
   "metadata": {},
   "source": [
    "Unfortunately, this training process takes slightly longer compared to earlier training loops (+- 30 min for 30 epochs). However, it's worthwhile to take some time to consider potential optimizations that could enhance its speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58484529",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 30\n",
    "learning_rate = 0.0001\n",
    "\n",
    "segmentation_model = SegmentationCNN(in_channels=3, classes=21)\n",
    "\n",
    "train_model_segmentation(segmentation_model, trainloader, nb_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize examples (No modifications needed)\n",
    "colors = {\n",
    "        0: (0, 0, 0),\n",
    "        1: (31, 119, 180),\n",
    "        2: (255, 127, 14),  \n",
    "        3: (44, 160, 44),   \n",
    "        4: (214, 39, 40), \n",
    "        5: (148, 103, 189),\n",
    "        6: (140, 86, 75),\n",
    "        7: (227, 119, 194),\n",
    "        8: (127, 127, 127),\n",
    "        9: (188, 189, 34),\n",
    "        10: (23, 190, 207),\n",
    "        11: (174, 199, 232),\n",
    "        12: (255, 187, 120),\n",
    "        13: (152, 223, 138),\n",
    "        14: (255, 152, 150),\n",
    "        15: (197, 176, 213),\n",
    "        16: (196, 156, 148),\n",
    "        17: (247, 182, 210),\n",
    "        18: (199, 199, 199),\n",
    "        19: (219, 219, 141),\n",
    "        255: (0, 0, 0)\n",
    "}\n",
    "\n",
    "class_names = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n",
    "    'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "def mask_to_rgb(mask, class_to_color):\n",
    "    \"\"\"\n",
    "    Converts a numpy mask with multiple classes indicated by integers to a color RGB mask.\n",
    "\n",
    "    Parameters:\n",
    "        mask (numpy.ndarray): The input mask where each integer represents a class.\n",
    "        class_to_color (dict): A dictionary mapping class integers to RGB color tuples.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: RGB mask where each pixel is represented as an RGB tuple.\n",
    "    \"\"\"\n",
    "    # Get dimensions of the input mask\n",
    "    height, width = mask.shape\n",
    "\n",
    "    # Initialize an empty RGB mask\n",
    "    rgb_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Iterate over each class and assign corresponding RGB color\n",
    "    for class_idx, color in class_to_color.items():\n",
    "        # Mask pixels belonging to the current class\n",
    "        class_pixels = mask == class_idx\n",
    "        # Assign RGB color to the corresponding pixels\n",
    "        rgb_mask[class_pixels] = color\n",
    "\n",
    "    return rgb_mask\n",
    "\n",
    "def visualize_segmentation(model, dataloader, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualizes segmentation results from a given model using a dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The segmentation model to visualize.\n",
    "        dataloader (torch.utils.data.DataLoader): Dataloader providing image-mask pairs.\n",
    "        num_examples (int, optional): Number of examples to visualize. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            if i >= num_examples:\n",
    "                break\n",
    "            \n",
    "            outputs = model(images)\n",
    "            outputs = torch.softmax(outputs, dim=1)\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "\n",
    "            images = images.numpy()\n",
    "            masks = masks.numpy()*255\n",
    "\n",
    "            predicted = predicted.numpy()\n",
    "\n",
    "            for j in range(images.shape[0]):\n",
    "                image = renormalize_image(images[j].transpose(1, 2, 0))\n",
    "\n",
    "                mask = masks[j].squeeze()\n",
    "                pred_mask = predicted[j]\n",
    "                                \n",
    "                # Convert mask and predicted mask to RGB for visualization\n",
    "                mask_rgb = mask_to_rgb(mask, colors)\n",
    "                pred_mask_rgb = mask_to_rgb(pred_mask, colors)\n",
    "                \n",
    "                # Get unique classes present in the ground truth and predicted masks\n",
    "                unique_classes_gt = np.unique(mask)\n",
    "                unique_classes_pred = np.unique(pred_mask)\n",
    "                \n",
    "                unique_classes_gt = np.delete(unique_classes_gt, [0, -1])\n",
    "                unique_classes_pred= np.delete(unique_classes_pred, 0)\n",
    "                \n",
    "                unique_classes_gt[unique_classes_gt == 255] = 0\n",
    "                unique_classes_pred[unique_classes_pred == 255] = 0\n",
    "                \n",
    "                \n",
    "                # Map class indices to class names from the VOC2012 dataset\n",
    "                classes_gt = [class_names[int(idx)] for idx in unique_classes_gt]\n",
    "                classes_pred = [class_names[int(idx)] for idx in unique_classes_pred]\n",
    "                \n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title('Image')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(mask_rgb)\n",
    "                plt.title(f'Ground Truth Mask Classes:\\n {classes_gt}')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(pred_mask_rgb)\n",
    "                plt.title(f'Predicted Mask Predicted Classes:\\n {classes_pred}')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "\n",
    "def renormalize_image(image):\n",
    "    \"\"\"\n",
    "    Renormalizes the image to its original range.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Image tensor to renormalize.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Renormalized image tensor.\n",
    "    \"\"\"\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]  \n",
    "    renormalized_image = image * std + mean\n",
    "    return renormalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91dc5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Think about results\n",
    "visualize_segmentation(segmentation_model, validationloader, num_examples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f284a96",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Q7. Which specific modifications or enhancements were implemented to improve the performance of the model?\n",
    "\n",
    "Q8. What are the primary challenges or obstacles encountered by the model when addressing the segmentation problem, and what possible solution can we use to tackle these challenges? Think about the classes the model already segmentens correctly and which classes are misted completely. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
